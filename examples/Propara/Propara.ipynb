{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hfaghihi/Framework/DomiKnowS/examples/Propara\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "# Please change the root to an absolute or relative path to DomiKnowS root.\n",
    "# In case relative path is used, consider the printed `CWD` as current working directory.\n",
    "root = '/home/hfaghihi/Framework/DomiKnowS'\n",
    "import sys\n",
    "sys.path.append(root)\n",
    "from typing import Any, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '37',\n",
       " 'entities': ['plant; animal'],\n",
       " 'steps': ['A plant of animal dies in a watery environment.',\n",
       "  'Is buried in mud and silt.',\n",
       "  'Soft tissues quickly decompose leaving behind hard bones or shells.',\n",
       "  'Over time sediment builds over the top and hardens into rock.',\n",
       "  'As the bone decays mineral seeps in replacing it.',\n",
       "  'Fossils are formed.'],\n",
       " 'entity_step': [[0.9683084487915039,\n",
       "   0.029291482642292976,\n",
       "   0.0024000774137675762],\n",
       "  [0.012266993522644043, 0.002779645612463355, 0.9849532842636108],\n",
       "  [0.006055360194295645, 0.0023835168685764074, 0.9915611147880554],\n",
       "  [0.0024350169114768505, 0.0022026486694812775, 0.9953623414039612],\n",
       "  [0.0015909943031147122, 0.0026168148033320904, 0.9957921504974365],\n",
       "  [0.0015094410628080368, 0.002839647000655532, 0.9956509470939636],\n",
       "  [0.0016313738888129592, 0.0032306257635354996, 0.9951379895210266]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"updated_test_data.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regr.data.reader import RegrReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProparaReader(RegrReader):\n",
    "    def getprocedureIDval(self, item):\n",
    "        return item['id']\n",
    "    def getentitiesval(self, item):\n",
    "        return item['entities']\n",
    "    def getstepsval(self, item):\n",
    "        num_steps = len(item['steps']) + 1\n",
    "        rel = torch.ones(num_steps,1)\n",
    "        return rel, [\"step 0 information\"].extend(item['steps'])\n",
    "    \n",
    "    def getnon_existenceval(self, item):\n",
    "        values = []\n",
    "        for step in range(len(item['steps']) + 1):\n",
    "            values.append([1 - item['entity_step'][step][2], item['entity_step'][step][2]])\n",
    "        return torch.tensor(values)\n",
    "            \n",
    "    def getknownval(self, item):\n",
    "        values = []\n",
    "        for step in range(len(item['steps']) + 1):\n",
    "            values.append([1 - item['entity_step'][step][0], item['entity_step'][step][0]])\n",
    "        return torch.tensor(values)\n",
    "    \n",
    "    def getunkownval(self, item):\n",
    "        values = []\n",
    "        for step in range(len(item['steps']) + 1):\n",
    "            values.append([1 - item['entity_step'][step][1], item['entity_step'][step][1]])\n",
    "        return torch.tensor(values)\n",
    "    \n",
    "    def getactionval(self, item):\n",
    "        action1s = torch.diag(torch.ones(len(item['steps']) + 1) )[:-1]\n",
    "        action2s = torch.diag(torch.ones(len(item['steps']) + 1) )[1:]\n",
    "        return action1s, action2s\n",
    "    \n",
    "    def getcreateval(self, item):\n",
    "        actions = []\n",
    "        for sid, step in enumerate(item['steps']):\n",
    "            o = 0\n",
    "            c = 0\n",
    "            d = 0\n",
    "            if sid == 0:\n",
    "                prev_state = item['entity_step'][sid]\n",
    "                continue\n",
    "            else:\n",
    "                o += (prev_state[0] * item['entity_step'][sid][0])\n",
    "                o += (prev_state[0] * item['entity_step'][sid][1])\n",
    "                o += (prev_state[1] * item['entity_step'][sid][0])\n",
    "                o += (prev_state[1] * item['entity_step'][sid][1])\n",
    "                o += (prev_state[2] * item['entity_step'][sid][2])\n",
    "                d += (prev_state[0] * item['entity_step'][sid][2])\n",
    "                d += (prev_state[1] * item['entity_step'][sid][2])\n",
    "                c += (prev_state[2] * item['entity_step'][sid][1])\n",
    "                c += (prev_state[2] * item['entity_step'][sid][0])\n",
    "                actions.append([1-c, c])\n",
    "                prev_state = item['entity_step'][sid]\n",
    "        return actions\n",
    "                    \n",
    "    def getdestroyval(self, item):\n",
    "        actions = []\n",
    "        for sid, step in enumerate(item['steps']):\n",
    "            o = 0\n",
    "            c = 0\n",
    "            d = 0\n",
    "            if sid == 0:\n",
    "                prev_state = item['entity_step'][sid]\n",
    "                continue\n",
    "            else:\n",
    "                o += (prev_state[0] * item['entity_step'][sid][0])\n",
    "                o += (prev_state[0] * item['entity_step'][sid][1])\n",
    "                o += (prev_state[1] * item['entity_step'][sid][0])\n",
    "                o += (prev_state[1] * item['entity_step'][sid][1])\n",
    "                o += (prev_state[2] * item['entity_step'][sid][2])\n",
    "                d += (prev_state[0] * item['entity_step'][sid][2])\n",
    "                d += (prev_state[1] * item['entity_step'][sid][2])\n",
    "                c += (prev_state[2] * item['entity_step'][sid][1])\n",
    "                c += (prev_state[2] * item['entity_step'][sid][0])\n",
    "                actions.append([1-d, d])\n",
    "                prev_state = item['entity_step'][sid]\n",
    "        return actions\n",
    "    \n",
    "    def getotherval(self, item):\n",
    "        actions = []\n",
    "        for sid, step in enumerate(item['steps']):\n",
    "            o = 0\n",
    "            c = 0\n",
    "            d = 0\n",
    "            if sid == 0:\n",
    "                prev_state = item['entity_step'][sid]\n",
    "                continue\n",
    "            else:\n",
    "                o += (prev_state[0] * item['entity_step'][sid][0])\n",
    "                o += (prev_state[0] * item['entity_step'][sid][1])\n",
    "                o += (prev_state[1] * item['entity_step'][sid][0])\n",
    "                o += (prev_state[1] * item['entity_step'][sid][1])\n",
    "                o += (prev_state[2] * item['entity_step'][sid][2])\n",
    "                d += (prev_state[0] * item['entity_step'][sid][2])\n",
    "                d += (prev_state[1] * item['entity_step'][sid][2])\n",
    "                c += (prev_state[2] * item['entity_step'][sid][1])\n",
    "                c += (prev_state[2] * item['entity_step'][sid][0])\n",
    "                actions.append([1-o, o])\n",
    "                prev_state = item['entity_step'][sid]\n",
    "        return actions\n",
    "    \n",
    "    def getbeforeval(self, item):\n",
    "        b1s = []\n",
    "        b2s = []\n",
    "        for step in range(len(item['steps']) + 1):\n",
    "            b1 = torch.zeros(len(item['steps']) + 1)\n",
    "            b1[step] = 1\n",
    "            for step1 in range(step+1, len(item['steps']) + 1):\n",
    "                b2 = torch.zeros(len(item['steps']) + 1)\n",
    "                b2[step1] = 1\n",
    "                b1s.append(b1)\n",
    "                b2s.append(b2)\n",
    "        return torch.stack(b1s), torch.stack(b2s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': (tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0.]]),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1.]])),\n",
       " 'before': (tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0.]]),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 1., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 0., 0., 0., 1.]])),\n",
       " 'create': [[0.9999638869012587, 3.6113098741282564e-05],\n",
       "  [0.9916881003214273, 0.008311899678572682],\n",
       "  [0.995401471146531, 0.004598528853469036],\n",
       "  [0.9958117052756262, 0.004188294724373799],\n",
       "  [0.9956692122445809, 0.004330787755419119]],\n",
       " 'destroy': [[0.017410671153128776, 0.9825893288468712],\n",
       "  [0.9850803377253793, 0.014919662274620674],\n",
       "  [0.9916002595678794, 0.008399740432120509],\n",
       "  [0.9953818490178498, 0.0046181509821502376],\n",
       "  [0.9958104908779757, 0.004189509122024364]],\n",
       " 'entities': ['plant; animal'],\n",
       " 'known': tensor([[0.0317, 0.9683],\n",
       "         [0.9877, 0.0123],\n",
       "         [0.9939, 0.0061],\n",
       "         [0.9976, 0.0024],\n",
       "         [0.9984, 0.0016],\n",
       "         [0.9985, 0.0015],\n",
       "         [0.9984, 0.0016]]),\n",
       " 'non_existence': tensor([[0.9976, 0.0024],\n",
       "         [0.0150, 0.9850],\n",
       "         [0.0084, 0.9916],\n",
       "         [0.0046, 0.9954],\n",
       "         [0.0042, 0.9958],\n",
       "         [0.0043, 0.9957],\n",
       "         [0.0049, 0.9951]]),\n",
       " 'other': [[0.9826255096993305, 0.017374490300669513],\n",
       "  [0.02323164670354705, 0.976768353296453],\n",
       "  [0.012998270449742777, 0.9870017295502572],\n",
       "  [0.0088064791177217, 0.9911935208822783],\n",
       "  [0.008520302116134415, 0.9914796978838656]],\n",
       " 'procedureID': '37',\n",
       " 'steps': (tensor([[1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.]]),\n",
       "  None),\n",
       " 'unkown': tensor([[0.9707, 0.0293],\n",
       "         [0.9972, 0.0028],\n",
       "         [0.9976, 0.0024],\n",
       "         [0.9978, 0.0022],\n",
       "         [0.9974, 0.0026],\n",
       "         [0.9972, 0.0028],\n",
       "         [0.9968, 0.0032]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReaderObjectsIterator = ProparaReader(\"updated_test_data.json\")\n",
    "next(iter(ReaderObjectsIterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file for dataNode is in: /home/hfaghihi/Framework/DomiKnowS/examples/Propara/datanode.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: Please use OrderedDict rather than dict to prevent unpredictable order of arguments.For this instance, OrderedDict([('arg1', 'step'), ('arg2', 'step')]) is used.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Please use OrderedDict rather than dict to prevent unpredictable order of arguments.For this instance, OrderedDict([('arg1', 'step'), ('arg2', 'step')]) is used.\n"
     ]
    }
   ],
   "source": [
    "from regr.graph import Graph, Concept, Relation\n",
    "from regr.graph.logicalConstrain import orL, andL, existsL, notL, atLeastL, atMostL, ifL, nandL\n",
    "\n",
    "Graph.clear()\n",
    "Concept.clear()\n",
    "Relation.clear()\n",
    "\n",
    "with Graph('global') as graph:\n",
    "    procedure = Concept(\"procedure\")\n",
    "    step = Concept(\"step\")\n",
    "    (procedure_contain_step, ) = procedure.contains(step)\n",
    "    entity = Concept(\"entity\")\n",
    "    non_existence = step(\"none_existence\")\n",
    "    unknown_loc = step(\"unknown_location\")\n",
    "    known_loc = step(\"known_location\")\n",
    "    before = Concept(\"before\")\n",
    "    (before_arg1, before_arg2) = before.has_a(arg1=step, arg2=step)\n",
    "    action = Concept(\"action\")\n",
    "    (action_arg1, action_arg2) = action.has_a(arg1=step, arg2=step)\n",
    "    create = action(\"create\")\n",
    "    destroy = action(\"create\")\n",
    "    other = action(\"create\")\n",
    "\n",
    "    nandL(create, destroy, other)\n",
    "    nandL(known_loc, unknown_loc, non_existence)\n",
    "    ifL(create, (\"x\", ), andL(non_existence, (\"x\", \"arg1\"), orL(known_loc, (\"x\", \"arg2\"), unknown_loc, (\"x\", \"arg2\"))))\n",
    "    ifL(destroy, (\"x\", ), andL(orL(known_loc, (\"x\", \"arg1\"), unknown_loc, (\"x\", \"arg1\")), non_existence, (\"x\", \"arg2\")))\n",
    "    atMostL(1, (\"x\", ), create, (\"x\", ))\n",
    "    atMostL(1, (\"x\", ), destroy, (\"x\", ))\n",
    "#     ifL(andL(create, (\"x\", ), destory, (\"y\", )), existL(before))\n",
    "    # No entity_step\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regr.sensor.pytorch.sensors import ReaderSensor, JointSensor\n",
    "from regr.sensor.pytorch.relation_sensors import EdgeSensor\n",
    "\n",
    "class EdgeReaderSensor(EdgeSensor, ReaderSensor):\n",
    "    def __init__(self, *pres, relation, mode=\"forward\", keyword=None, **kwargs):\n",
    "        super().__init__(*pres, relation=relation, mode=mode, **kwargs)\n",
    "        self.keyword = keyword\n",
    "        self.data = None\n",
    "        \n",
    "class JoinReaderSensor(JointSensor, ReaderSensor):\n",
    "    pass\n",
    "            \n",
    "class JoinEdgeReaderSensor(JointSensor, EdgeReaderSensor):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regr.sensor.pytorch.sensors import ReaderSensor\n",
    "from regr.program import LearningBasedProgram\n",
    "from regr.program.model.pytorch import PoiModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def model_declaration():\n",
    "    from graph import graph, procedure, step, entity, non_existence, unknown_loc, known_loc, action, create, destroy, other\n",
    "    from graph import procedure_contain_step, action_arg1, action_arg2, before_arg1, before_arg2\n",
    "\n",
    "    graph.detach()\n",
    "\n",
    "    # --- City\n",
    "    procedure['id'] = ReaderSensor(keyword='procedureID')\n",
    "    step[procedure_contain_step.forward, 'text'] = JoinEdgeReaderSensor(procedure['id'], keyword='steps', relation=procedure_contain_step, mode=\"forward\")\n",
    "    # word[step_contains_word, 'raw'] = ReaderSensor(keyword='words')\n",
    "#     entity['raw'] = ReaderSensor(keyword='entities')\n",
    "    step[non_existence] = ReaderSensor('text', keyword='non_existence')\n",
    "    step[unknown_loc] = ReaderSensor('text', keyword='known')\n",
    "    step[known_loc] = ReaderSensor('text', keyword='unkown')\n",
    "    step[non_existence] = ReaderSensor('text', keyword='non_existence')\n",
    "    step[unknown_loc] = ReaderSensor('text', keyword='known')\n",
    "    step[known_loc] = ReaderSensor('text', keyword='unkown')\n",
    "    action[action_arg1.forward, action_arg2.forward] = JoinReaderSensor(step['text'], keyword='action')\n",
    "    action[create] = ReaderSensor(action_arg1.forward, action_arg2.forward, keyword='create')\n",
    "    action[destroy] = ReaderSensor(action_arg1.forward, action_arg2.forward, keyword='destroy')\n",
    "    action[other] = ReaderSensor(action_arg1.forward, action_arg2.forward, keyword='other')\n",
    "    action[create] = ReaderSensor(action_arg1.forward, action_arg2.forward, keyword='create')\n",
    "    action[destroy] = ReaderSensor(action_arg1.forward, action_arg2.forward, keyword='destroy')\n",
    "    action[other] = ReaderSensor(action_arg1.forward, action_arg2.forward, keyword='other')\n",
    "    before[before_arg1, before_arg2] = JoinReaderSensor(step['text'], keyword=\"before\")\n",
    "    before[before_arg1, before_arg2] = JoinReaderSensor(step['text'], keyword=\"before\")\n",
    "    program = LearningBasedProgram(graph, **{\n",
    "        'Model': PoiModel,\n",
    "#         'poi': (known_loc, unknown_loc, non_existence, other, destroy, create),\n",
    "        'loss': None,\n",
    "        'metric': None,\n",
    "    })\n",
    "    return program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    from graph import graph, procedure, word, step, entity, entity_step, entity_step_word, location_start, location_end, non_existence, unknown_loc, known_loc, action, create, destroy, other\n",
    "    from graph import entity_of_step, entity_of_step_word, step_contains_word, step_of_entity, step_of_entity_word, word_of_entity_step, procedure_contain_step, action_arg1, action_arg2\n",
    "\n",
    "    # set logger level to see training and testing logs\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    lbp = model_declaration()\n",
    "\n",
    "    dataset = ProparaReader(file='updated_test_data.json')  # Adding the info on the reader\n",
    "\n",
    "#     lbp.test(dataset, device='auto')\n",
    "\n",
    "    for datanode in lbp.populate(dataset, device=\"cpu\"):\n",
    "        print('datanode:', datanode)\n",
    "#         print('Spam:', datanode.getAttribute(Spam).softmax(-1))\n",
    "#         print('Regular:', datanode.getAttribute(Regular).softmax(-1))\n",
    "        datanode.inferILPConstrains('create', 'destroy', 'other', 'non_existence', \"known_location\", 'unknown_location', fun=None)\n",
    "        print('datanode:', datanode)\n",
    "#         print('inference spam:', datanode.getAttribute(Spam, 'ILP'))\n",
    "#         print('inference regular:', datanode.getAttribute(Regular, 'ILP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datanode: procedure 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-71f165beedba>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#         print('Spam:', datanode.getAttribute(Spam).softmax(-1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#         print('Regular:', datanode.getAttribute(Regular).softmax(-1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdatanode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferILPConstrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'create'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'destroy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'other'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'non_existence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"known_location\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unknown_location'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datanode:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatanode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#         print('inference spam:', datanode.getAttribute(Spam, 'ILP'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/DomiKnowS/regr/graph/dataNode.py\u001b[0m in \u001b[0;36minferILPConstrains\u001b[0;34m(self, fun, epsilon, minimizeObjective, *_conceptsRelations)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mmyilpOntSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_candidatesID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraphResultsForPhraseToken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraphResultsForPhraseRelation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraphResultsForTripleRelations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhardConstrains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates_currentConceptOrRelation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__prepareILPData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_conceptsRelations\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdnFun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getProbability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmyilpOntSolver\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/DomiKnowS/regr/graph/dataNode.py\u001b[0m in \u001b[0;36m__prepareILPData\u001b[0;34m(self, dnFun, fun, epsilon, *_conceptsRelations)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;31m# Get candidates (dataNodes or relation relationName for the concept) from the graph starting from the current data node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m             \u001b[0mcurrentCandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcurrentConceptOrRelation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DataNode__Logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0mconceptsRelations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentConceptOrRelation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/DomiKnowS/regr/graph/concept.py\u001b[0m in \u001b[0;36mcandidates\u001b[0;34m(self, root_data, query, logger)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasetype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# homogenous base type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_base_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
