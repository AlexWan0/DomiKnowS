{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "norman-profit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hfaghihi/Framework/new/DomiKnowS/examples\n",
      "root Folder Absoloute path:  /home/hfaghihi/Framework/new/DomiKnowS\n"
     ]
    }
   ],
   "source": [
    "import os,sys,inspect\n",
    "import json\n",
    "import torch\n",
    "currentdir = os.path.dirname(os.getcwd())\n",
    "print(currentdir)\n",
    "# parent_dir = os.path.abspath(os.path.join(currentdir, os.pardir))\n",
    "root = os.path.dirname(currentdir)\n",
    "print(\"root Folder Absoloute path: \", root)\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(root)\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import __main__\n",
    "\n",
    "__main__.__file__=\"main.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "transparent-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regr.data.reader import RegrReader\n",
    "import torch\n",
    "\n",
    "\n",
    "class SudokuReader(RegrReader):\n",
    "    def parse_file(self):\n",
    "        return [[\"random\"] * 1]\n",
    "    def getidval(self, item):\n",
    "        return [1]\n",
    "    def getwhole_sudokuval(self, item):\n",
    "        base  = 3\n",
    "        side  = base*base\n",
    "\n",
    "        # pattern for a baseline valid solution\n",
    "        def pattern(r,c): return (base*(r%base)+r//base+c)%side\n",
    "\n",
    "        # randomize rows, columns and numbers (of valid base pattern)\n",
    "        from random import sample\n",
    "        def shuffle(s): return sample(s,len(s)) \n",
    "        rBase = range(base) \n",
    "        rows  = [ g*base + r for g in shuffle(rBase) for r in shuffle(rBase) ] \n",
    "        cols  = [ g*base + c for g in shuffle(rBase) for c in shuffle(rBase) ]\n",
    "        nums  = shuffle(range(1,base*base+1))\n",
    "\n",
    "        # produce board using randomized baseline pattern\n",
    "        board = [ [nums[pattern(r,c)] for c in cols] for r in rows ]\n",
    "        squares = side*side\n",
    "        empties = squares * 3//4\n",
    "        for p in sample(range(squares),empties):\n",
    "            board[p//side][p%side] = 0\n",
    "        return torch.tensor(board)\n",
    "            \n",
    "    \n",
    "    def getsizeval(self, item):\n",
    "        return 9, 9\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "skilled-surface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [1], 'size': (9, 9), 'whole_sudoku': tensor([[0, 7, 3, 0, 0, 0, 0, 5, 6],\n",
      "        [0, 8, 0, 5, 6, 0, 3, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [7, 0, 0, 0, 8, 0, 0, 0, 0],\n",
      "        [8, 0, 1, 0, 0, 6, 0, 4, 0],\n",
      "        [0, 0, 0, 0, 0, 9, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 4],\n",
      "        [0, 0, 0, 9, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 5, 6, 0, 7, 0, 0, 0]])}\n",
      "tensor([[0, 1],\n",
      "        [0, 2],\n",
      "        [0, 7],\n",
      "        [0, 8],\n",
      "        [1, 1],\n",
      "        [1, 3],\n",
      "        [1, 4],\n",
      "        [1, 6],\n",
      "        [2, 8],\n",
      "        [3, 0],\n",
      "        [3, 4],\n",
      "        [4, 0],\n",
      "        [4, 2],\n",
      "        [4, 5],\n",
      "        [4, 7],\n",
      "        [5, 5],\n",
      "        [6, 8],\n",
      "        [7, 3],\n",
      "        [8, 2],\n",
      "        [8, 3],\n",
      "        [8, 5]])\n",
      "(tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 3, 3, 4, 4, 4, 4, 5, 6, 7, 8, 8, 8]), tensor([1, 2, 7, 8, 1, 3, 4, 6, 8, 0, 4, 0, 2, 5, 7, 5, 8, 3, 2, 3, 5]))\n",
      "tensor([7, 3, 5, 6, 8, 5, 6, 3, 1, 7, 8, 8, 1, 6, 4, 9, 4, 9, 5, 6, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main.py:4: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729006826/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n"
     ]
    }
   ],
   "source": [
    "trainreader = SudokuReader(\"randn\", type=\"raw\")\n",
    "item = next(iter(trainreader))\n",
    "print(item)\n",
    "indices = (item['whole_sudoku'] != 0).nonzero()\n",
    "print(indices)\n",
    "print(torch.where(item['whole_sudoku'] != 0))\n",
    "print(item['whole_sudoku'][indices[:, 0], indices[:, 1]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prospective-clock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file for dataNode is in: /home/hfaghihi/Framework/new/DomiKnowS/examples/sudoku/logs/datanode.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main.py:26: UserWarning: Please use OrderedDict rather than dict to prevent unpredictable order of arguments.For this instance, OrderedDict([('arg1', 'empty_entry'), ('arg2', 'empty_entry')]) is used.\n",
      "main.py:31: UserWarning: Please use OrderedDict rather than dict to prevent unpredictable order of arguments.For this instance, OrderedDict([('arg1', 'empty_entry'), ('arg2', 'empty_entry')]) is used.\n",
      "main.py:35: UserWarning: Please use OrderedDict rather than dict to prevent unpredictable order of arguments.For this instance, OrderedDict([('arg1', 'empty_entry'), ('arg2', 'empty_entry')]) is used.\n"
     ]
    }
   ],
   "source": [
    "from regr.graph import Graph, Concept, Relation\n",
    "from regr.graph.logicalConstrain import orL, andL, existsL, notL, atLeastL, atMostL, ifL, nandL, V, exactL\n",
    "from regr.graph import EnumConcept\n",
    "\n",
    "\n",
    "Graph.clear()\n",
    "Concept.clear()\n",
    "Relation.clear()\n",
    "\n",
    "with Graph('global') as graph:\n",
    "    sudoku = Concept(\"sodoku\")\n",
    "    \n",
    "#     empty_entries = Concept(name=\"empty_entries\")\n",
    "#     fixed_entries = Concept(name=\"fixed_entries\")\n",
    "    \n",
    "#     (sudoku_empty, sudoku_fixed) = sudoku.has_a(empty_entries, fixed_entries)\n",
    "    \n",
    "    empty_entry = Concept(name='empty_entry')\n",
    "    (empty_rel, ) = sudoku.contains(empty_entry)\n",
    "    \n",
    "#     fixed_entry = Concept(name='fixed_entry')\n",
    "#     (fixed_rel, ) = fixed_entries.contains(fixed_entry)\n",
    "    \n",
    "    same_row = Concept(name=\"same_row\")\n",
    "#     same_row_mixed = Concept(name=\"same_row_mixed\")\n",
    "    (same_row_arg1, same_row_arg2) = same_row.has_a(arg1=empty_entry, arg2=empty_entry)\n",
    "#     (same_row_mixed_arg1, same_row_mixed_arg2) = same_row_mixed.has_a(arg1=empty_entry, arg2=fixed_entry)\n",
    "    \n",
    "    same_col = Concept(name=\"same_col\")\n",
    "#     same_col_mixed = Concept(name=\"same_col_mixed\")\n",
    "    (same_col_arg1, same_col_arg2) = same_col.has_a(arg1=empty_entry, arg2=empty_entry)\n",
    "#     (same_col_mixed_arg1, same_col_mixed_arg2) = same_col_mixed.has_a(arg1=empty_entry, arg2=fixed_entry)\n",
    "\n",
    "    same_table = Concept(name=\"same_table\")\n",
    "    (same_table_arg1, same_table_arg2) = same_col.has_a(arg1=empty_entry, arg2=empty_entry)\n",
    "    \n",
    "    empty_entry_label = empty_entry(name=\"empty_entry_label\", ConceptClass=EnumConcept, values=[\"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\"])\n",
    "\n",
    "\n",
    "    ### Constriants\n",
    "    \n",
    "    for val in [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]:\n",
    "        ### No same number in the same row between empty entries and empty entries\n",
    "        ifL(getattr(empty_entry_label, f'v{val}')('x'), notL(\n",
    "            existsL(\n",
    "                andL(\n",
    "                    same_row('z', path=(\"x\", same_row_arg1.reversed)), getattr(empty_entry_label, f'v{val}')('y', path=(\"z\", same_row_arg2))\n",
    "                ))\n",
    "        ))\n",
    "        \n",
    "        ### No same number in the same column between empty entries and empty entries\n",
    "        ifL(getattr(empty_entry_label, f'v{val}')('x'), notL(\n",
    "            existsL(\n",
    "                andL(\n",
    "                    same_col('z', path=(\"x\", same_col_arg1.reversed)), getattr(empty_entry_label, f'v{val}')('y', path=(\"z\", same_col_arg2))\n",
    "                ))\n",
    "        ))\n",
    "        \n",
    "        ### No same number in the same table between empty entries and empty entries\n",
    "        ifL(getattr(empty_entry_label, f'v{val}')('x'), notL(\n",
    "            existsL(\n",
    "                andL(\n",
    "                    same_table('z', path=(\"x\", same_table_arg1.reversed)), getattr(empty_entry_label, f'v{val}')('y', path=(\"z\", same_table_arg2))\n",
    "                ))\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regulation-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "class Conv2dSame(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bias=True, padding_layer=torch.nn.ReflectionPad2d):\n",
    "        \"\"\"It only support square kernels and stride=1, dilation=1, groups=1.\"\"\"\n",
    "        super(Conv2dSame, self).__init__()\n",
    "        ka = kernel_size // 2\n",
    "        kb = ka - 1 if kernel_size % 2 == 0 else ka\n",
    "        self.net = nn.Sequential(\n",
    "            padding_layer((ka,kb,ka,kb)),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, bias=bias),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class SudokuCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SudokuCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(Conv2dSame(1,512,3), #1\n",
    "                                         Conv2dSame(512,512,3),#2\n",
    "                                         Conv2dSame(512,512,3),#3\n",
    "                                         Conv2dSame(512,512,3),#4\n",
    "                                         Conv2dSame(512,512,3),#5\n",
    "                                         Conv2dSame(512,512,3),#6\n",
    "                                         Conv2dSame(512,512,3),#7\n",
    "                                         Conv2dSame(512,512,3),#8\n",
    "                                         Conv2dSame(512,512,3),#9\n",
    "                                         Conv2dSame(512,512,3),#10\n",
    "                                         Conv2dSame(512,512,3),#11\n",
    "                                         Conv2dSame(512,512,3),#12\n",
    "                                         Conv2dSame(512,512,3),#13\n",
    "                                         Conv2dSame(512,512,3),#14\n",
    "                                         Conv2dSame(512,512,3))#15\n",
    "        self.last_conv = nn.Conv2d(512, 9, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.last_conv(x)\n",
    "        x = x.permute(0,2,3,1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "capable-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regr.sensor.pytorch.sensors import FunctionalSensor, JointSensor, ReaderSensor, FunctionalReaderSensor\n",
    "from regr.sensor.pytorch.learners import ModuleLearner\n",
    "from regr.sensor.pytorch.relation_sensors import CompositionCandidateSensor\n",
    "from regr.sensor.pytorch.query_sensor import DataNodeReaderSensor\n",
    "\n",
    "class JointFunctionalReaderSensor(JointSensor, FunctionalReaderSensor):\n",
    "    pass\n",
    "\n",
    "\n",
    "# def getempties(*prev, data):\n",
    "#     rows, cols = torch.where(data == 0)\n",
    "#     rel = torch.ones(1, 1)\n",
    "#     return [rows], [cols]\n",
    "    \n",
    "def getfixed(*prev, data):\n",
    "    rows, cols = torch.where(data != 0)\n",
    "    fix = torch.zeros(data.shape)\n",
    "    vals = torch.ones(data.shape) * -1\n",
    "    for i, j in zip(rows.detach().tolist(), cols.detach().tolist()):\n",
    "        fix[i][j] = 1\n",
    "        vals[i][j] = data[i][j]\n",
    "        \n",
    "        \n",
    "    return fix.reshape(fix.shape[0]*fix.shape[1]), vals.reshape(vals.shape[0]*vals.shape[1])\n",
    "\n",
    "def makeSoduko(*prev, data):\n",
    "    num_rows = data[0]\n",
    "    num_cols = data[1]\n",
    "    rows = torch.arange(num_rows).unsqueeze(-1)\n",
    "    rows = rows.repeat(1,num_cols).reshape(num_rows*num_cols)\n",
    "    \n",
    "    cols = torch.arange(num_rows)\n",
    "    cols = cols.unsqueeze(0).repeat(num_rows, 1).reshape(num_rows*num_cols)\n",
    "    \n",
    "    rel = torch.ones(data[0]*data[1], 1)\n",
    "    \n",
    "    return rows, cols, rel\n",
    "\n",
    "def getlabel(*prev, data):\n",
    "    rows, cols = torch.where(data != 0)\n",
    "    vals = torch.ones(data.shape) * -100\n",
    "    for i, j in zip(rows.detach().tolist(), cols.detach().tolist()):\n",
    "        vals[i][j] = data[i][j] - 1\n",
    "        \n",
    "        \n",
    "    return vals.reshape(vals.shape[0]*vals.shape[1])\n",
    "    \n",
    "    \n",
    "def createSudoku(*prev, data):\n",
    "    return [1]\n",
    "\n",
    "sudoku['index'] = FunctionalReaderSensor(keyword='size', forward=createSudoku)\n",
    "    \n",
    "empty_entry['rows', 'cols', empty_rel] = JointFunctionalReaderSensor(sudoku['index'], keyword='size', forward=makeSoduko)\n",
    "empty_entry['fixed', 'val'] = JointFunctionalReaderSensor('rows', 'cols', empty_rel, keyword='whole_sudoku', forward=getfixed)\n",
    "\n",
    "class FunctionalModuleLearner(ModuleLearner):\n",
    "    def forward(self, *inputs):\n",
    "        inputs = list(inputs)\n",
    "        inputs[0] = inputs[0].view(1, 1,9, 9)\n",
    "        return self.module(*inputs)\n",
    "\n",
    "empty_entry[empty_entry_label] = FunctionalModuleLearner('val', module=SudokuCNN())\n",
    "empty_entry[empty_entry_label] = FunctionalReaderSensor(keyword='whole_sudoku', label=True, forward=getlabel)\n",
    "# fixed_entries['rows1', 'cols'] = JointFunctionalReaderSensor(keyword='whole_sudoku', forward=getfixed)\n",
    "\n",
    "\n",
    "### What kind of model should we use for learning the entries? Because it should be aware of all other decision to make the correct decision, otherwise it is impossible for the model to learn good weights.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "democratic-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regr.program import POIProgram, SolverPOIProgram, IMLProgram, CallbackProgram\n",
    "from regr.program.callbackprogram import ProgramStorageCallback\n",
    "from regr.program.metric import MacroAverageTracker, PRF1Tracker, DatanodeCMMetric\n",
    "from regr.program.loss import NBCrossEntropyLoss, NBCrossEntropyIMLoss\n",
    "\n",
    "program = SolverPOIProgram(\n",
    "        graph, poi=(sudoku, empty_entry, ), inferTypes=['local/argmax'],\n",
    "        loss=MacroAverageTracker(NBCrossEntropyLoss()),\n",
    "        metric={\n",
    "            'argmax': PRF1Tracker(DatanodeCMMetric('local/argmax'))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "exempt-ocean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sodoku 0\n",
      "[empty_entry 0, empty_entry 1, empty_entry 2, empty_entry 3, empty_entry 4, empty_entry 5, empty_entry 6, empty_entry 7, empty_entry 8, empty_entry 9, empty_entry 10, empty_entry 11, empty_entry 12, empty_entry 13, empty_entry 14, empty_entry 15, empty_entry 16, empty_entry 17, empty_entry 18, empty_entry 19, empty_entry 20, empty_entry 21, empty_entry 22, empty_entry 23, empty_entry 24, empty_entry 25, empty_entry 26, empty_entry 27, empty_entry 28, empty_entry 29, empty_entry 30, empty_entry 31, empty_entry 32, empty_entry 33, empty_entry 34, empty_entry 35, empty_entry 36, empty_entry 37, empty_entry 38, empty_entry 39, empty_entry 40, empty_entry 41, empty_entry 42, empty_entry 43, empty_entry 44, empty_entry 45, empty_entry 46, empty_entry 47, empty_entry 48, empty_entry 49, empty_entry 50, empty_entry 51, empty_entry 52, empty_entry 53, empty_entry 54, empty_entry 55, empty_entry 56, empty_entry 57, empty_entry 58, empty_entry 59, empty_entry 60, empty_entry 61, empty_entry 62, empty_entry 63, empty_entry 64, empty_entry 65, empty_entry 66, empty_entry 67, empty_entry 68, empty_entry 69, empty_entry 70, empty_entry 71, empty_entry 72, empty_entry 73, empty_entry 74, empty_entry 75, empty_entry 76, empty_entry 77, empty_entry 78, empty_entry 79, empty_entry 80]\n"
     ]
    }
   ],
   "source": [
    "for datanode in program.populate(trainreader):\n",
    "    print(datanode)\n",
    "    \n",
    "    print(datanode.getChildDataNodes(conceptName=empty_entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-ladder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
