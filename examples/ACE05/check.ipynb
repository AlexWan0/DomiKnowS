{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hfaghihi/Framework/DomiKnowS/examples/ACE05\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "# Please change the root to an absolute or relative path to DomiKnowS root.\n",
    "# In case relative path is used, consider the printed `CWD` as current working directory.\n",
    "root = '/home/hfaghihi/Framework/DomiKnowS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file for dataNode is in: /home/hfaghihi/Framework/DomiKnowS/examples/ACE05/datanode.log\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from regr.sensor.pytorch.sensors import ReaderSensor, ConstantSensor, FunctionalSensor, FunctionalReaderSensor, TorchEdgeSensor\n",
    "\n",
    "\n",
    "class MultiLevelReaderSensor(ConstantSensor):\n",
    "    def __init__(self, *pres, keyword=None, edges=None, label=False, device='auto'):\n",
    "        super().__init__(*pres, data=None, edges=edges, label=label, device=device)\n",
    "        self.keyword = keyword\n",
    "\n",
    "    def fill_data(self, data_item):\n",
    "        try:\n",
    "            if isinstance(self.keyword, tuple):\n",
    "                self.data = (self.fetch_key(data_item, keyword) for keyword in self.keyword)\n",
    "            else:\n",
    "                self.data = self.fetch_key(data_item, self.keyword)\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\"The key you requested from the reader doesn't exist: %s\" % str(e))\n",
    "\n",
    "    def fetch_key(self, data_item, key):\n",
    "        data = []\n",
    "        if \".\" in key:\n",
    "            keys = key.split(\".\")\n",
    "            items = data_item\n",
    "            loop = 0\n",
    "            direct_loop = True\n",
    "            for key in keys:\n",
    "                if key == \"*\":\n",
    "                    loop += 1\n",
    "                    if loop == 1:\n",
    "                        keys = items.keys()\n",
    "                        items = [items[key] for key in keys]\n",
    "                    if loop > 1:\n",
    "                        keys = [item.keys() for item in items]\n",
    "                        new_items = []\n",
    "                        for index, item in enumerate(items):\n",
    "                            for index1, key in enumerate(keys[index]):\n",
    "                                new_items.append(item[key])\n",
    "                        items = new_items\n",
    "                else:\n",
    "                    if loop == 0:\n",
    "                        items = items[key]\n",
    "                    if loop > 0:\n",
    "                        items = [it[key] for it in items]\n",
    "            data = items\n",
    "        else:\n",
    "            data = data_item[key]\n",
    "\n",
    "        return data\n",
    "        \n",
    "        \n",
    "    def forward(self, *_) -> Any:\n",
    "        if isinstance(self.keyword, tuple) and isinstance(self.data, tuple):\n",
    "            return (super().forward(data) for data in self.data)\n",
    "        else:\n",
    "            return super().forward(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from regr.program import POIProgram\n",
    "from regr.sensor.pytorch.sensors import ReaderSensor, ConstantSensor, FunctionalSensor, FunctionalReaderSensor, TorchEdgeSensor\n",
    "from regr.sensor.pytorch.learners import ModuleLearner\n",
    "from regr.sensor.pytorch.query_sensor import CandidateSensor, CandidateRelationSensor\n",
    "\n",
    "from sensors.tokenizers import TokenizerEdgeSensor\n",
    "from models import Tokenizer, BERT, SpanClassifier, cartesian_concat, token_to_span_candidate, span_candidate_emb, span_label, span_emb, find_is_a\n",
    "\n",
    "\n",
    "def model(graph):\n",
    "    graph.detach()\n",
    "\n",
    "    ling_graph = graph['linguistic']\n",
    "    ace05_graph = graph['ACE05']\n",
    "    entities_graph = ace05_graph['Entities']\n",
    "    relations_graph = ace05_graph['Relations']\n",
    "    events_graph = ace05_graph['Events']\n",
    "\n",
    "    document = ling_graph['document']\n",
    "    token = ling_graph['token']\n",
    "    span_candidate = ling_graph['span_candidate']\n",
    "    span_annotation = ling_graph['span_annotation']\n",
    "    span = ling_graph['span']\n",
    "    document_contains_token = document.relate_to(token)[0]\n",
    "    span_contains_token = span.relate_to(token)[0]\n",
    "    span_is_span_candidate = span.relate_to(span_candidate)[0]\n",
    "\n",
    "    document['index'] = ReaderSensor(keyword='text')\n",
    "    document_contains_token['forward'] = TokenizerEdgeSensor('index', mode='forward', to=('index', 'ids', 'offset'), tokenizer=Tokenizer())\n",
    "    token['emb'] = ModuleLearner('ids', module=BERT())\n",
    "\n",
    "    span_annotation['extent'] = MultiLevelReaderSensor(keyword=\"spans.*.mentions.*.extent.start\")\n",
    "    span_annotation['extent'] = ConstantSensor(data=[\"a\", \"b\", \"c\", \"d\"])\n",
    "\n",
    "    program = POIProgram(graph, poi=(token, span_candidate, span,))\n",
    "\n",
    "    return program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[652, 836, 660, 110, 782, 870, 910, 122, 178, 266, 315, 417, 429, 608, 682, 146, 162, 727, 769, 791, 215, 295, 632, 81, 228, 323, 339, 407, 501, 40, 89]\n",
      "[658, 856, 680, 113, 785, 872, 913, 124, 188, 268, 318, 420, 431, 610, 684, 157, 171, 731, 774, 795, 225, 302, 637, 86, 229, 328, 349, 411, 509, 59, 108]\n"
     ]
    }
   ],
   "source": [
    "from ace05.reader import Reader, DictReader\n",
    "import config\n",
    "\n",
    "sensor = MultiLevelReaderSensor(keyword=\"spans.*.mentions.*.head.start\")\n",
    "sensor1 = MultiLevelReaderSensor(keyword=\"spans.*.mentions.*.head.end\")\n",
    "traint_reader = DictReader(config.path, list_path=config.list_path, type='train', status=config.status)\n",
    "sensor.fill_data(next(iter(traint_reader)))\n",
    "sensor1.fill_data(next(iter(traint_reader)))\n",
    "print(sensor.data)\n",
    "print(sensor1.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
