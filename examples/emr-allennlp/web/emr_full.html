<!--
This file was programmatically generated by
github.com/allenai/allennlp/tutorials/tagger/convert.py.
Any manual changes you make to it will be overwritten
the next time the file is generated. Please make your changes
to the original python file or to the convert.py script, 
as appropriate.
-->
<h1>
<a id="user-content-example-entity-mention-relation-emr" class="anchor" href="#example-entity-mention-relation-emr" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Example: Entity-Mention-Relation (EMR)</h1>
<h2>
<a id="user-content-pipeline" class="anchor" href="#pipeline" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline</h2>
<p>This example follows the pipeline we discussed in our preliminary paper.</p>
<ol>
<li>Ontology Declaration</li>
<li>Model Declaration</li>
<li>Explicit inference</li>
</ol>
<div id="annotated-code">
  <!-- Code Blocks -->
  <div class="annotated-code__pane annotated-code__pane--code-container">
<div class="annotated-code__code-block" id="c0">
{% highlight python %}
from regr.sensor.allennlp.sensor import SequenceSensor, TokenInSequenceSensor, LabelSensor, CartesianProductSensor
from regr.sensor.allennlp.learner import W2VLearner, RNNLearner, LogisticRegressionLearner
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c1">
{% highlight python %}
from regr.graph.allennlp import AllenNlpGraph
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c2">
{% highlight python %}
from .data import Conll04SensorReader as Reader
from .config import Config
from .utils import seed
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c3">
{% highlight python %}
def ontology_declaration():
    from .graph import graph
    return graph
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c4">
{% highlight python %}
def model_declaration(graph, config):
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c5">
{% highlight python %}
    graph.detach()
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c6">
{% highlight python %}
    sentence = graph['linguistic/sentence']
    phrase = graph['linguistic/phrase']
    pair = graph['linguistic/pair']
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c7">
{% highlight python %}
    people = graph['application/people']
    organization = graph['application/organization']
    location = graph['application/location']
    other = graph['application/other']
    o = graph['application/O']
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c8">
{% highlight python %}
    work_for = graph['application/work_for']
    located_in = graph['application/located_in']
    live_in = graph['application/live_in']
    orgbase_on = graph['application/orgbase_on']
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c9">
{% highlight python %}
    reader = Reader()
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c10">
{% highlight python %}
    sentence['raw'] = SequenceSensor(reader, 'sentence')
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c11">
{% highlight python %}
    phrase['raw'] = TokenInSequenceSensor(sentence['raw'])
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c12">
{% highlight python %}
    phrase['w2v'] = W2VLearner(config.embedding_dim, phrase['raw'])
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c13">
{% highlight python %}
    phrase['emb'] = RNNLearner(config.embedding_dim, phrase['w2v'])
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c14">
{% highlight python %}
    pair['emb'] = CartesianProductSensor(phrase['emb'])
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c15">
{% highlight python %}
    people['label'] = LabelSensor(reader, 'Peop', output_only=True)
    organization['label'] = LabelSensor(reader, 'Org', output_only=True)
    location['label'] = LabelSensor(reader, 'Loc', output_only=True)
    other['label'] = LabelSensor(reader, 'Other', output_only=True)
    o['label'] = LabelSensor(reader, 'O', output_only=True)
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c16">
{% highlight python %}
    people['label'] = LogisticRegressionLearner(config.embedding_dim * 2, phrase['emb'])
    organization['label'] = LogisticRegressionLearner(config.embedding_dim * 2, phrase['emb'])
    location['label'] = LogisticRegressionLearner(config.embedding_dim * 2, phrase['emb'])
    other['label'] = LogisticRegressionLearner(config.embedding_dim * 2, phrase['emb'])
    o['label'] = LogisticRegressionLearner(config.embedding_dim * 2, phrase['emb'])
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c17">
{% highlight python %}
    work_for['label'] = LabelSensor(reader, 'Work_For', output_only=True)
    live_in['label'] = LabelSensor(reader, 'Live_In', output_only=True)
    located_in['label'] = LabelSensor(reader, 'Located_In', output_only=True)
    orgbase_on['label'] = LabelSensor(reader, 'OrgBased_In', output_only=True)
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c18">
{% highlight python %}
    work_for['label'] = LogisticRegressionLearner(config.embedding_dim * 4, pair['emb'])
    live_in['label'] = LogisticRegressionLearner(config.embedding_dim * 4, pair['emb'])
    located_in['label'] = LogisticRegressionLearner(config.embedding_dim * 4, pair['emb'])
    orgbase_on['label'] = LogisticRegressionLearner(config.embedding_dim * 4, pair['emb'])
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c19">
{% highlight python %}
    lbp = AllenNlpGraph(graph)
    return lbp
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c20">
{% highlight python %}
def main():
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c21">
{% highlight python %}
    graph = ontology_declaration()
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c22">
{% highlight python %}
    lbp = model_declaration(graph, Config.Model)
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c23">
{% highlight python %}
    seed()
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c24">
{% highlight python %}
    lbp.train(Config.Data, Config.Train)
{% endhighlight %}
</div>
<div class="annotated-code__code-block" id="c25">
{% highlight python %}
    lbp.save('/tmp/emr')
{% endhighlight %}
</div>
</div>
    <!-- END Code Blocks -->

    <!-- Annotations -->
    <div class="annotated-code__pane annotated-code__pane--annotations-container">
        <ul id="annotated-code__annotations">
<li class="annotation" id="a0"><p>With <code>regr</code>, we assign sensors to properties of concept. There are two types of sensor: <code>Sensor</code>s and <code>Learner</code>s. <code>Sensor</code> is the more general term, while a <code>Learner</code> is a <code>Sensor</code> with learnable parameters.</p>
</li>
<li class="annotation" id="a1"><p><code>AllenNlpGraph</code> is a special subclass of <code>Graph</code> that wrap a <code>Graph</code> and add computational functionalities to it.</p>
</li>
<li class="annotation" id="a2"><p>There are a few other components that is needed in common machine learning models. * <code>Conll04SensorReader</code> is a AllenNLP <code>DatasetReader</code>. We added a bit magics to make it more compatible with <code>regr</code>. See <code>data.py</code> for details. * <code>Config</code> contain configurations for model, data, and training. * <code>seed</code> is a useful function the reset random seed of all involving sub-systems: Python, numpy, and PyTorch, to make the performance of training consistent, as a demo.</p>
</li>
<li class="annotation" id="a3"><p>"<em>Ontology Declaration</em>" is the first step in our pipeline. A graph of the concepts, representing the ontology in this application, is declared. It can be compile from standard ontology formats like <code>OWL</code>, writen with python grammar directly, or combine both way. Here we just import the graph from <code>graph.py</code>. Please also refer to <code>graph.py</code> for details.</p>
</li>
<li class="annotation" id="a4"><p>"<em>Model Declaration</em>" comes after "Ontology Declaration" in our pipeline. Sensors and learners are connected to the graph, what wrap the graph with functionalities to retieve data, forward computing, learning from error, and inference during all those processes. <code>graph</code> is a <code>Graph</code> object retieve from the "Ontology Declaration". <code>config</code> is configurations relatred to the model.</p>
</li>
<li class="annotation" id="a5"><p><code>Graph</code> objects has some kind of global variables. Use <code>.detach()</code> to reset them to avoid baffling error.</p>
</li>
<li class="annotation" id="a6"><p>Retrieve concepts that are needed in this model. Notice that these concepts are already well defined in <code>graph.py</code>. Here we just retrieve them to use them as python variables. <code>sentence</code>, <code>phrase</code>, and <code>pair</code> are basic linguistic concepts in this demo.</p>
</li>
<li class="annotation" id="a7"><p><code>people</code>, <code>organization</code>, <code>location</code>, <code>other</code>, and <code>O</code> are entities we want to extract in this demo.</p>
</li>
<li class="annotation" id="a8"><p><code>people</code>, <code>organization</code>, <code>location</code>, <code>other</code>, and <code>O</code> are entities we want to extract in this demo.</p>
</li>
<li class="annotation" id="a9"><p>Create a <code>Conll04SensorReader</code> instance, to be assigned with properties, and allow the model to get corresponding data from it.</p>
</li>
<li class="annotation" id="a10"><p>The most important part in "Model Declaration" is to connect sensors (and learners) to the graph. We start with linguistic concepts. <code>SequenceSensor</code> provides the ability to read from a <code>TextField</code> in AllenNLP. It take two arguments, firstly the reader to read with, and secondly a <code>key</code> for the reader to refer to correct <code>TextField</code>.</p>
</li>
<li class="annotation" id="a11"><p><code>TokenInSequenceSensor</code> provides the ability to index tokens in a <code>TextField</code>. Notice that the Conll04 dataset comes with phrase tokenized sentences. Thus this is already a phrase-based sentence. <code>TokenInSequenceSensor</code> takes the sentence <code>TextField</code> here and insert a token field to it. Please also refer to AllenNLP <code>TextField</code> document for complicated relationship of it and its tokens.</p>
</li>
<li class="annotation" id="a12"><p><code>W2VLearner</code> converts index-based <code>phrase['raw']</code> into vectors by "word2vec" module that is widely applied in Deep Learning with NLP tasks. The first argument specify the output dimensions of this module, that is the dimension of output vectors. And the second argument tells the learner from where it convert. Notice that this is a learner which imply there are trainable parameters in this <code>learner</code>. <code>Learner</code>s are just <code>Sensor</code>s with parameters. In this example, this implies we want to update this "word2vec" dictionary.</p>
</li>
<li class="annotation" id="a13"><p><code>RNNLearner</code> takes a sequence of representations as input, encodes them with recurrent nerual networks (RNN), like LSTM or GRU, and provides the encoded output. Here we encode the word2vec output further with an RNN. The first argument indicates the dimensions of internal representations, and the second one incidates we will encode the output of <code>phrase['w2v']</code>. More optional arguments are avaliable, like <code>bidirectional</code> defaulted to <code>True</code> for context from both sides, and <code>dropout</code> defaulted to <code>0.5</code> for tackling overfitting.</p>
</li>
<li class="annotation" id="a14"><p><code>CartesianProductSensor</code> is a <code>Sensor</code> that takes the representation from <code>phrase['emb']</code>, makes all possible combination of them, and generates a concatenating result for each combination. This process takes no parameters. But there is still a PyTorch module associated with it.</p>
</li>
<li class="annotation" id="a15"><p>Then we connect properties with ground-truth from <code>reader</code>. <code>LabelSensor</code> takes the <code>reader</code> as argument to provide the ground-truth data. The second argument indicates the key we used for each lable in reader. The last keyword argument <code>output_only</code> indicates that these sensors are not to be used with forward computation.</p>
</li>
<li class="annotation" id="a16"><p>We connect properties with learners that generate predictions. Notice that we connect the predicting <code>Learner</code>s to the same properties as "ground-truth" <code>Sensor</code>s. Multiple assignment is a feature in <code>regr</code> to allow each property to have multiple sources. Value from different sources will be compared, to generate inconsistency error. The training of this model is then based on this inconsistency error. In this example, "ground-truth" <code>Sensor</code>s has no parameters to be trained, while predicting <code>Learner</code>s have all sets of paramters to be trained. The error also propagate backward through the computational path to all modules as assigned above. Here we use <code>LogisticRegressionLearner</code>s, which is binary classifiers. Notice the first argument, the "input dimention", takes a <code>* 2</code> because the output from <code>phrase['emb']</code> is bidirectional, having two times dimentions. The second argument is base on what the prediction will be made. The constructors make individule modules for them with seperated parameters, though they take same arguments.</p>
</li>
<li class="annotation" id="a17"><p>We repeat these on composed-concepts. There is nothing different in usage thought they are higher ordered concepts.</p>
</li>
<li class="annotation" id="a18"><p>We also connect the predictors for composed-concepts. Notice the first argument, the "input dimention", takes a <code>* 4</code> because <code>pair['emb']</code> from <code>CartesianProductSensor</code> has double dimention again over <code>phrase['emb']</code>.</p>
</li>
<li class="annotation" id="a19"><p>Lastly, we wrap these graph with <code>AllenNlpGraph</code> functionalities to get the full learning based program.</p>
</li>
<li class="annotation" id="a20"><p>The main entrance of the program.</p>
</li>
<li class="annotation" id="a21"><ol>
<li>"Ontology Declaration" to get a graph, as a partial program.</li>
</ol>
</li>
<li class="annotation" id="a22"><ol start="2">
<li>"Model Declaration" to connect sensors and learners and get the full program.</li>
</ol>
</li>
<li class="annotation" id="a23"><ol start="3">
<li>Train and save the model "Explicit inference" is done automatically in every call to the model. To have better reproducibility, we initial the random seeds of all subsystems.</li>
</ol>
</li>
<li class="annotation" id="a24"><p>Train the model with inference functionality inside.</p>
</li>
<li class="annotation" id="a25"><p>Save the model, including vocabulary use to index the tokens.</p>
</li>
</ul>
  </div><!-- END Annotations -->
</div><!-- END Annotated Code -->
<p>This example show a full pipeline how to work with <code>regr</code>.</p>

 {% include more-tutorials.html %}
